{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9cUWaOMTCqbPu6hrpWqsy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manishkancharla/Statistics-Basic-Assignment/blob/main/Statistics_Advance_1_i_Assignment_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Explain the properties of the F-distribution?\n",
        "\n",
        "A) Non-Negativity:\n",
        "\n",
        "The F-distribution is only defined for positive values (\n",
        "𝐹\n",
        "≥\n",
        "0\n",
        "F≥0) because it is derived from a ratio of variances, which cannot be negative.\n",
        "\n",
        "Shape of the Distribution:\n",
        "\n",
        "The shape of the F-distribution depends on two parameters: the degrees of freedom for the numerator (\n",
        "𝑑\n",
        "1\n",
        "d\n",
        "1\n",
        "​\n",
        " ) and the denominator (\n",
        "𝑑\n",
        "2\n",
        "d\n",
        "2\n",
        "​\n",
        " ).\n",
        "As\n",
        "𝑑\n",
        "1\n",
        "d\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑑\n",
        "2\n",
        "d\n",
        "2\n",
        "​\n",
        "  increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
        "\n",
        "Asymmetry:\n",
        "\n",
        "The F-distribution is right-skewed, especially for smaller degrees of freedom. The skewness decreases as\n",
        "𝑑\n",
        "1\n",
        "d\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑑\n",
        "2\n",
        "d\n",
        "2\n",
        "​\n",
        "  increase.\n",
        "Dependence on Degrees of Freedom:\n",
        "\n",
        "The mean and variance of the F-distribution depend on\n",
        "𝑑\n",
        "1\n",
        "d\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑑\n",
        "2\n",
        "d\n",
        "2\n",
        "​\n",
        " :\n",
        "Mean:\n",
        "𝜇\n",
        "=\n",
        "𝑑\n",
        "2\n",
        "𝑑\n",
        "2\n",
        "−\n",
        "2\n",
        "μ=\n",
        "d\n",
        "2\n",
        "​\n",
        " −2\n",
        "d\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        "  (for\n",
        "𝑑\n",
        "2\n",
        ">\n",
        "2\n",
        "d\n",
        "2\n",
        "​\n",
        " >2)\n",
        "\n",
        "Variance:\n",
        "𝜎\n",
        "2\n",
        "=\n",
        "2\n",
        "𝑑\n",
        "2\n",
        "2\n",
        "(\n",
        "𝑑\n",
        "1\n",
        "+\n",
        "𝑑\n",
        "2\n",
        "−\n",
        "2\n",
        ")\n",
        "𝑑\n",
        "1\n",
        "(\n",
        "𝑑\n",
        "2\n",
        "−\n",
        "2\n",
        ")\n",
        "2\n",
        "(\n",
        "𝑑\n",
        "2\n",
        "−\n",
        "4\n",
        ")\n",
        "σ\n",
        "2\n",
        " =\n",
        "d\n",
        "1\n",
        "​\n",
        " (d\n",
        "2\n",
        "​\n",
        " −2)\n",
        "2\n",
        " (d\n",
        "2\n",
        "​\n",
        " −4)\n",
        "2d\n",
        "2\n",
        "2\n",
        "​\n",
        " (d\n",
        "1\n",
        "​\n",
        " +d\n",
        "2\n",
        "​\n",
        " −2)\n",
        "​\n",
        "  (for\n",
        "𝑑\n",
        "2\n",
        ">\n",
        "4\n",
        "d\n",
        "2\n",
        "​\n",
        " >4)\n",
        "\n",
        "\n",
        "Right-Tailed Nature:\n",
        "\n",
        "The F-distribution is primarily used in right-tailed tests because it measures ratios of variances, and larger ratios indicate greater differences between groups.\n",
        "\n",
        "\n",
        "Applications:\n",
        "\n",
        "ANOVA (Analysis of Variance): Used to compare multiple group means by examining variance.\n",
        "\n",
        "Regression Analysis: Used to test the significance of the overall model fit.\n",
        "Hypothesis Testing for Variances: Tests if two population variances are equal.\n",
        "Relation to Other Distributions:\n",
        "\n",
        "If\n",
        "𝑋\n",
        "∼\n",
        "𝜒\n",
        "2\n",
        "(\n",
        "𝑑\n",
        "1\n",
        ")\n",
        "X∼χ\n",
        "2\n",
        " (d\n",
        "1\n",
        "​\n",
        " ) and\n",
        "𝑌\n",
        "∼\n",
        "𝜒\n",
        "2\n",
        "(\n",
        "𝑑\n",
        "2\n",
        ")\n",
        "Y∼χ\n",
        "2\n",
        " (d\n",
        "2\n",
        "​\n",
        " ), and\n",
        "𝑋\n",
        "X and\n",
        "𝑌\n",
        "Y are independent, then:\n",
        "𝐹\n",
        "=\n",
        "𝑋\n",
        "𝑑\n",
        "1\n",
        "𝑌\n",
        "𝑑\n",
        "2\n",
        "F=\n",
        "d\n",
        "2\n",
        "​\n",
        "\n",
        "Y\n",
        "​\n",
        "\n",
        "d\n",
        "1\n",
        "​\n",
        "\n",
        "X\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "follows an\n",
        "𝐹\n",
        "(\n",
        "𝑑\n",
        "1\n",
        ",\n",
        "𝑑\n",
        "2\n",
        ")\n",
        "F(d\n",
        "1\n",
        "​\n",
        " ,d\n",
        "2\n",
        "​\n",
        " ) distribution.\n"
      ],
      "metadata": {
        "id": "lYRpZN9V88f_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "A) The F-distribution is widely used in statistical tests that involve comparing variances or testing relationships between multiple variables. Below are the key tests where the F-distribution is applied:\n",
        "\n",
        "1. Analysis of Variance (ANOVA)\n",
        "Purpose:\n",
        "ANOVA tests whether the means of three or more groups are significantly different by comparing the variance between the groups to the variance within the groups.\n",
        "\n",
        "Why Use the F-Distribution:\n",
        "ANOVA relies on the ratio of two variances:\n",
        "𝐹\n",
        "=\n",
        "Variance Between Groups\n",
        "Variance Within Groups\n",
        "F=\n",
        "Variance Within Groups\n",
        "Variance Between Groups\n",
        "​\n",
        "\n",
        "Since variances are non-negative and the F-distribution models the ratio of two variances, it is the appropriate distribution for this test.\n",
        "Example:\n",
        "Testing whether the mean test scores of students in three different teaching methods are significantly different.\n",
        "\n",
        "2. Regression Analysis\n",
        "Purpose:\n",
        "The F-test in regression analysis is used to test the overall significance of the model—whether at least one predictor variable has a non-zero coefficient.\n",
        "\n",
        "Why Use the F-Distribution:\n",
        "The F-statistic in regression compares the explained variance (due to the model) to the unexplained variance (residual error):\n",
        "𝐹\n",
        "=\n",
        "Explained Variance / Degrees of Freedom\n",
        "Residual Variance / Degrees of Freedom\n",
        "F=\n",
        "Residual Variance / Degrees of Freedom\n",
        "Explained Variance / Degrees of Freedom\n",
        "​\n",
        "\n",
        "The F-distribution is appropriate because it tests whether the proportion of variance explained by the model is significantly greater than the proportion due to random error.\n",
        "Example:\n",
        "Testing whether a linear regression model significantly explains the variation in house prices based on predictors like size, location, and age.\n",
        "\n",
        "3. Hypothesis Testing for Variance Equality (F-Test)\n",
        "Purpose:\n",
        "The F-test for equality of variances compares the variances of two independent samples to determine if they are significantly different.\n",
        "\n",
        "Why Use the F-Distribution:\n",
        "The F-statistic is calculated as the ratio of two sample variances:\n",
        "𝐹\n",
        "=\n",
        "𝑆\n",
        "1\n",
        "2\n",
        "𝑆\n",
        "2\n",
        "2\n",
        "F=\n",
        "S\n",
        "2\n",
        "2\n",
        "​\n",
        "\n",
        "S\n",
        "1\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "The F-distribution is suitable because it describes the distribution of this ratio under the null hypothesis that the variances are equal.\n",
        "Example:\n",
        "Testing whether the variance in test scores for two different teaching methods is the same.\n",
        "\n",
        "4. MANOVA (Multivariate Analysis of Variance)\n",
        "Purpose:\n",
        "MANOVA extends ANOVA to test the difference in group means for multiple dependent variables simultaneously.\n",
        "\n",
        "Why Use the F-Distribution:\n",
        "The test involves ratios of variance and covariance matrices, and the F-distribution is used to determine the significance of the results.\n",
        "Example:\n",
        "Testing whether different diets have an effect on both weight loss and cholesterol reduction.\n",
        "\n",
        "5. Comparing Nested Models (Likelihood Ratio Test)\n",
        "Purpose:\n",
        "The F-test is used to compare nested models (e.g., a full model versus a reduced model) to determine if additional predictors significantly improve the model fit.\n",
        "\n",
        "Why Use the F-Distribution:\n",
        "The test evaluates the ratio of the improvement in fit to the residual variance, which follows an F-distribution.\n",
        "Example:\n",
        "Testing whether adding an interaction term to a regression model significantly improves its explanatory power."
      ],
      "metadata": {
        "id": "ZzEwQ7pS88Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?\n",
        "\n",
        "A) The F-test is used to compare the variances of two populations, and it relies on the following assumptions to produce valid results:\n",
        "\n",
        "1. Populations are Normally Distributed\n",
        "Each of the two populations being compared should follow a normal distribution.\n",
        "If the populations deviate significantly from normality, the F-test may lead to incorrect conclusions because the F-distribution assumes normality.\n",
        "2. Independence of Samples\n",
        "The samples drawn from each population must be independent of each other.\n",
        "Independence ensures that the variance estimates are unbiased and do not influence each other.\n",
        "3. Measurement Scale\n",
        "The data must be measured on an interval or ratio scale.\n",
        "This ensures that the concept of variance is meaningful and can be mathematically valid.\n",
        "4. Random Sampling\n",
        "The samples must be randomly selected from the respective populations.\n",
        "Random sampling helps ensure that the samples are representative of their populations and reduces selection bias.\n",
        "5. Homoscedasticity (in ANOVA context)\n",
        "While the F-test is often used to test for homoscedasticity (equal variances), when used in ANOVA or regression, the F-test assumes equal variances among groups.\n",
        "6. Non-Negativity of Variances\n",
        "Variances must be non-negative because the F-test statistic involves the ratio of variances, which would be invalid for negative values.\n",
        "F-Test Statistic\n",
        "The F-statistic is calculated as:\n",
        "\n",
        "𝐹\n",
        "=\n",
        "𝑆\n",
        "1\n",
        "2\n",
        "𝑆\n",
        "2\n",
        "2\n",
        "F=\n",
        "S\n",
        "2\n",
        "2\n",
        "​\n",
        "\n",
        "S\n",
        "1\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑆\n",
        "1\n",
        "2\n",
        "S\n",
        "1\n",
        "2\n",
        "​\n",
        "  = Variance of the first sample (assumed to be larger, for a one-tailed test)\n",
        "𝑆\n",
        "2\n",
        "2\n",
        "S\n",
        "2\n",
        "2\n",
        "​\n",
        "  = Variance of the second sample\n",
        "Under the null hypothesis (\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " ), the two population variances are equal, and the F-statistic follows an\n",
        "𝐹\n",
        "(\n",
        "𝑑\n",
        "1\n",
        ",\n",
        "𝑑\n",
        "2\n",
        ")\n",
        "F(d\n",
        "1\n",
        "​\n",
        " ,d\n",
        "2\n",
        "​\n",
        " ) distribution, where\n",
        "𝑑\n",
        "1\n",
        "d\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑑\n",
        "2\n",
        "d\n",
        "2\n",
        "​\n",
        "  are the degrees of freedom of the two samples.\n",
        "\n",
        "Example\n",
        "If you are comparing the variances of test scores from two teaching methods:\n",
        "\n",
        "Check if both sets of scores are approximately normally distributed (e.g., using a normality test like Shapiro-Wilk).\n",
        "Ensure that the samples are randomly selected and independent.\n",
        "Compute the sample variances and apply the F-test."
      ],
      "metadata": {
        "id": "pOaVtEh788Np"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)  What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "A) Purpose of ANOVA\n",
        "Analysis of Variance (ANOVA) is a statistical method used to determine whether there are statistically significant differences between the means of three or more groups.\n",
        "\n",
        "Primary Goal: To test the null hypothesis that all group means are equal:\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "1\n",
        "=\n",
        "𝜇\n",
        "2\n",
        "=\n",
        "⋯\n",
        "=\n",
        "𝜇\n",
        "𝑘\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ\n",
        "1\n",
        "​\n",
        " =μ\n",
        "2\n",
        "​\n",
        " =⋯=μ\n",
        "k\n",
        "​\n",
        "\n",
        "where\n",
        "𝑘\n",
        "k is the number of groups.\n",
        "Alternative Hypothesis: At least one group mean is different from the others.\n",
        "ANOVA achieves this by comparing the variance between groups (explained variance) to the variance within groups (unexplained variance).\n",
        "\n",
        "\n",
        "Purpose of ANOVA and Its Difference from a t-test\n",
        "Purpose of ANOVA\n",
        "Analysis of Variance (ANOVA) is a statistical method used to determine if there are significant differences between the means of three or more groups. The primary goal is to assess whether the variation between group means is greater than the variation within the groups, which might suggest that at least one group mean is significantly different.\n",
        "\n",
        "Key Applications of ANOVA:\n",
        "Comparing Group Means: Determine if different treatments, conditions, or groups produce significantly different outcomes.\n",
        "Testing Multiple Factors: Assess the effect of multiple independent variables (e.g., two-way ANOVA).\n",
        "Identifying Variability Sources: Separate and analyze variability between and within groups."
      ],
      "metadata": {
        "id": "gi-mUZDD88Fp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups?\n",
        "\n",
        "A) When to Use One-Way ANOVA\n",
        "One-way ANOVA is appropriate when you need to compare the means of three or more independent groups to determine if there is a statistically significant difference between their means.\n",
        "\n",
        "Conditions for Using One-Way ANOVA:\n",
        "You have one categorical independent variable with three or more levels (e.g., treatment groups, age groups).\n",
        "You have one continuous dependent variable (e.g., test scores, weight).\n",
        "The groups are independent (no repeated measures or paired data).\n",
        "The data meets the assumptions of normality and homogeneity of variance.\n",
        "Why Use One-Way ANOVA Instead of Multiple t-Tests?\n",
        "1. Reduces the Risk of Type I Error\n",
        "Type I Error: The probability of falsely rejecting the null hypothesis (\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " ) when it is true.\n",
        "Performing multiple t-tests for each pair of groups inflates the cumulative Type I error rate. For example, comparing 4 groups with t-tests requires 6 comparisons:\n",
        "Number of comparisons\n",
        "=\n",
        "𝑘\n",
        "(\n",
        "𝑘\n",
        "−\n",
        "1\n",
        ")\n",
        "2\n",
        ",\n",
        "\n",
        "where\n",
        "𝑘\n",
        "=\n",
        "number of groups.\n",
        "Number of comparisons=\n",
        "2\n",
        "k(k−1)\n",
        "​\n",
        " ,where k=number of groups.\n",
        "Each t-test has a Type I error rate (\n",
        "𝛼\n",
        "α), and multiple tests increase the overall error:\n",
        "Cumulative Type I error\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "𝑛\n",
        "Cumulative Type I error=1−(1−α)\n",
        "n\n",
        "\n",
        "ANOVA performs a single test, maintaining the desired Type I error rate (e.g.,\n",
        "𝛼\n",
        "=\n",
        "0.05\n",
        "α=0.05).\n",
        "2. More Efficient\n",
        "One-way ANOVA evaluates all group means simultaneously in one test.\n",
        "This reduces computational effort compared to performing multiple pairwise t-tests.\n",
        "3. Provides Overall Insights\n",
        "ANOVA determines if any group differs significantly from the others.\n",
        "It avoids the piecemeal approach of pairwise t-tests, focusing on the overall differences.\n",
        "4. Required for Post-hoc Tests\n",
        "If ANOVA finds a significant difference, post-hoc tests (e.g., Tukey’s HSD, Bonferroni) can be used to identify specific group differences while controlling for Type I error.\n"
      ],
      "metadata": {
        "id": "codIzoJh879g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "A) Partitioning Variance in ANOVA\n",
        "In Analysis of Variance (ANOVA), total variance in the data is partitioned into two components:\n",
        "\n",
        "Between-Group Variance (Variance Due to Treatment): Measures how much the group means differ from the overall mean.\n",
        "Within-Group Variance (Variance Due to Error): Measures the variability within each group, caused by random noise or individual differences.\n",
        "This partitioning allows us to determine whether the differences between group means are statistically significant or simply due to random variation.\n",
        "\n",
        "1. Total Variance\n",
        "The total variance measures the overall variability in the data, calculated as the deviations of individual data points from the overall mean (\n",
        "𝑋\n",
        "ˉ\n",
        "overall\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "overall\n",
        "​\n",
        " ).\n",
        "\n",
        "Total Sum of Squares (SST)\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑖\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "𝑗\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        "overall\n",
        ")\n",
        "2\n",
        "Total Sum of Squares (SST)=\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " (X\n",
        "ij\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "overall\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑋\n",
        "𝑖\n",
        "𝑗\n",
        "X\n",
        "ij\n",
        "​\n",
        " : Data point in group\n",
        "𝑖\n",
        "i, individual\n",
        "𝑗\n",
        "j.\n",
        "𝑋\n",
        "ˉ\n",
        "overall\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "overall\n",
        "​\n",
        " : Overall mean of all data.\n",
        "𝑘\n",
        "k: Number of groups.\n",
        "𝑛\n",
        "𝑖\n",
        "n\n",
        "i\n",
        "​\n",
        " : Number of data points in group\n",
        "𝑖\n",
        "i.\n",
        "2. Between-Group Variance\n",
        "The between-group variance quantifies the variability of group means (\n",
        "𝑋\n",
        "ˉ\n",
        "𝑖\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "i\n",
        "​\n",
        " ) relative to the overall mean (\n",
        "𝑋\n",
        "ˉ\n",
        "overall\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "overall\n",
        "​\n",
        " ). It reflects the differences between group means.\n",
        "\n",
        "Between-Group Sum of Squares (SSB)\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑖\n",
        "(\n",
        "𝑋\n",
        "ˉ\n",
        "𝑖\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        "overall\n",
        ")\n",
        "2\n",
        "Between-Group Sum of Squares (SSB)=\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "​\n",
        " n\n",
        "i\n",
        "​\n",
        " (\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "i\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "overall\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑋\n",
        "ˉ\n",
        "𝑖\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "i\n",
        "​\n",
        " : Mean of group\n",
        "𝑖\n",
        "i.\n",
        "𝑛\n",
        "𝑖\n",
        "n\n",
        "i\n",
        "​\n",
        " : Number of observations in group\n",
        "𝑖\n",
        "i.\n",
        "This value is influenced by the number of groups (\n",
        "𝑘\n",
        "k) and their sample sizes.\n",
        "\n",
        "3. Within-Group Variance\n",
        "The within-group variance quantifies the variability of individual data points around their respective group mean (\n",
        "𝑋\n",
        "ˉ\n",
        "𝑖\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "i\n",
        "​\n",
        " ). It reflects random noise or individual differences within groups.\n",
        "\n",
        "Within-Group Sum of Squares (SSW)\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑖\n",
        "(\n",
        "𝑋\n",
        "𝑖\n",
        "𝑗\n",
        "−\n",
        "𝑋\n",
        "ˉ\n",
        "𝑖\n",
        ")\n",
        "2\n",
        "Within-Group Sum of Squares (SSW)=\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " (X\n",
        "ij\n",
        "​\n",
        " −\n",
        "X\n",
        "ˉ\n",
        "  \n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Relationship Between Components\n",
        "The total variability (\n",
        "SST\n",
        "SST) is the sum of the between-group variability (\n",
        "SSB\n",
        "SSB) and within-group variability (\n",
        "SSW\n",
        "SSW):\n",
        "\n",
        "SST\n",
        "=\n",
        "SSB\n",
        "+\n",
        "SSW\n",
        "SST=SSB+SSW\n",
        "Contribution to the F-Statistic\n",
        "The F-statistic compares the variance between groups to the variance within groups. It is calculated as:\n",
        "\n",
        "𝐹\n",
        "=\n",
        "Mean Square Between (MSB)\n",
        "Mean Square Within (MSW)\n",
        "F=\n",
        "Mean Square Within (MSW)\n",
        "Mean Square Between (MSB)\n",
        "​\n",
        "\n",
        "Mean Square Calculations:\n",
        "Mean Square Between (MSB):\n",
        "MSB\n",
        "=\n",
        "SSB\n",
        "𝑘\n",
        "−\n",
        "1\n",
        "MSB=\n",
        "k−1\n",
        "SSB\n",
        "​\n",
        "\n",
        "Where\n",
        "𝑘\n",
        "k is the number of groups.\n",
        "\n",
        "Mean Square Within (MSW):\n",
        "MSW\n",
        "=\n",
        "SSW\n",
        "𝑁\n",
        "−\n",
        "𝑘\n",
        "MSW=\n",
        "N−k\n",
        "SSW\n",
        "​\n",
        "\n",
        "Where\n",
        "𝑁\n",
        "N is the total number of observations across all groups.\n",
        "\n",
        "Interpretation of the F-Statistic:\n",
        "If the F-statistic is significantly large, it suggests that the between-group variance is much larger than the within-group variance, indicating that group means are likely different.\n",
        "The significance of the F-statistic is determined by comparing it to a critical value from the F-distribution, given the degrees of freedom for MSB (\n",
        "𝑘\n",
        "−\n",
        "1\n",
        "k−1) and MSW (\n",
        "𝑁\n",
        "−\n",
        "𝑘\n",
        "N−k).\n",
        "Example: Visualizing the Components\n",
        "Between-group variance: Explains systematic differences caused by group treatments.\n",
        "Within-group variance: Reflects random noise or unexplained variability within groups.\n",
        "The F-statistic quantifies whether the between-group variability is large enough, relative to within-group variability, to reject the null hypothesis (\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " : group means are equal)."
      ],
      "metadata": {
        "id": "B6MISIlw872Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "A) Classical (Frequentist) ANOVA vs. Bayesian ANOVA\n",
        "Both classical and Bayesian ANOVA are statistical methods used to compare group means. However, they differ fundamentally in their philosophical approach, handling of uncertainty, parameter estimation, and hypothesis testing.\n",
        "\n",
        "1. Handling of Uncertainty\n",
        "Classical ANOVA:\n",
        "Focus on Null Hypothesis Testing: The classical approach centers on testing whether the group means are equal (\n",
        "𝐻\n",
        "0\n",
        ":\n",
        "𝜇\n",
        "1\n",
        "=\n",
        "𝜇\n",
        "2\n",
        "=\n",
        "⋯\n",
        "=\n",
        "𝜇\n",
        "𝑘\n",
        "H\n",
        "0\n",
        "​\n",
        " :μ\n",
        "1\n",
        "​\n",
        " =μ\n",
        "2\n",
        "​\n",
        " =⋯=μ\n",
        "k\n",
        "​\n",
        " ).\n",
        "Frequentist Probability: Uncertainty is interpreted as the long-run frequency of events. Results are expressed in terms of p-values, which indicate the probability of observing the data (or something more extreme) if\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        "  is true.\n",
        "Fixed Parameters: Group means and variances are considered fixed, unknown quantities.\n",
        "Bayesian ANOVA:\n",
        "Focus on Probability Distributions: Bayesian ANOVA models uncertainty about parameters explicitly through probability distributions.\n",
        "Bayesian Probability: Uncertainty is interpreted as a degree of belief. Results are expressed as posterior probabilities, which represent the probability of a hypothesis or parameter value given the observed data.\n",
        "Random Parameters: Group means and variances are treated as random variables with prior distributions.\n",
        "\n",
        "2. Parameter Estimation\n",
        "Classical ANOVA:\n",
        "Point Estimates: Estimates of parameters (e.g., group means) are based on the data alone. Variability in the data is accounted for by confidence intervals around these point estimates.\n",
        "No Prior Information: The classical approach does not incorporate prior knowledge or beliefs about parameter values.\n",
        "Bayesian ANOVA:\n",
        "Posterior Distributions: Parameters are estimated using posterior distributions, which combine prior information (if available) with the observed data via Bayes' theorem:\n",
        "Posterior\n",
        "∝\n",
        "Likelihood\n",
        "×\n",
        "Prior\n",
        "Posterior∝Likelihood×Prior\n",
        "Incorporates Priors: Bayesian ANOVA allows the inclusion of prior knowledge or beliefs about parameters, which can influence the results.\n",
        "\n",
        "3. Hypothesis Testing\n",
        "Classical ANOVA:\n",
        "Null Hypothesis Testing: Tests whether all group means are equal. If the p-value is less than the significance level (\n",
        "𝛼\n",
        "α), the null hypothesis is rejected.\n",
        "Binary Decision: Results in a binary outcome: reject or fail to reject\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " .\n",
        "F-Statistic: Uses the F-statistic to compare between-group variance to within-group variance. A significant F-statistic suggests at least one group mean is different.\n",
        "Bayesian ANOVA:\n",
        "Posterior Probabilities: Provides probabilities for hypotheses (e.g.,\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        "  and\n",
        "𝐻\n",
        "1\n",
        "H\n",
        "1\n",
        "​\n",
        " ) rather than binary decisions.\n",
        "Model Comparison: Often compares models using metrics like Bayes factors, which quantify evidence in favor of one model over another.\n",
        "Credible Intervals: Instead of p-values, Bayesian ANOVA provides credible intervals, which indicate the range of parameter values with a certain probability (e.g., 95%).\n",
        "\n",
        "4. Interpretation of Results\n",
        "Classical ANOVA:\n",
        "P-Values: A small p-value indicates evidence against\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        " , but it does not provide a direct probability for\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        "  being true.\n",
        "Confidence Intervals: Reflect the range of plausible values for a parameter under repeated sampling.\n",
        "Bayesian ANOVA:\n",
        "Posterior Probability: Provides a direct probability of\n",
        "𝐻\n",
        "0\n",
        "H\n",
        "0\n",
        "​\n",
        "  being true, given the data.\n",
        "Credible Intervals: Reflect the range of plausible parameter values based on the posterior distribution.\n",
        "\n",
        "5. Computational Complexity\n",
        "Classical ANOVA:\n",
        "Closed-Form Solutions: Involves straightforward calculations based on sums of squares and degrees of freedom.\n",
        "Less Computationally Intensive: Requires fewer computational resources.\n",
        "Bayesian ANOVA:\n",
        "Simulation-Based Methods: Often requires Markov Chain Monte Carlo (MCMC) or other numerical techniques to estimate posterior distributions.\n",
        "More Computationally Intensive: Requires more time and resources, especially for complex models.\n",
        "\n",
        "6. Assumptions\n",
        "Both methods rely on similar assumptions (e.g., normality, independence, homogeneity of variance). However:\n",
        "\n",
        "Classical ANOVA is more sensitive to violations of these assumptions.\n",
        "Bayesian ANOVA can incorporate flexible models and priors to handle assumption violations more gracefully.\n"
      ],
      "metadata": {
        "id": "hghE0MTO87uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) You have two sets of data representing the incomes of two different professions1\n",
        "\n",
        "Profession A: [48, 52, 55, 60, 62'\n",
        "\n",
        "V Profession B: [45, 50, 55, 52, 47]\n",
        "\n",
        "Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison?\n",
        "\n"
      ],
      "metadata": {
        "id": "C0Als4Mf87mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Define the data for each profession\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Perform the F-test\n",
        "f_statistic, p_value = f_oneway(profession_a, profession_b)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a statistically significant difference in variances between the two professions' incomes.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no statistically significant difference in variances between the two professions' incomes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnL-d8JzAd8S",
        "outputId": "8fb0e8f2-0797-452b-8398-56e7d55c2e40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 3.232989690721649\n",
            "p-value: 0.10987970118946545\n",
            "Fail to reject the null hypothesis. There is no statistically significant difference in variances between the two professions' incomes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "\n",
        "Region A: [160, 162, 165, 158, 164']\n",
        "\n",
        "Region B: [172, 175, 170, 168, 174']\n",
        "\n",
        "Region C: [180, 182, 179, 185, 183']\n",
        "\n",
        "Task: Write Python code to perform the one-way ANOVA and interpret the results.\n",
        "\n",
        "Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value?\n",
        "\n"
      ],
      "metadata": {
        "id": "UQUSQ_g-86hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Define the data for each region\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a statistically significant difference in average heights between the three regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no statistically significant difference in average heights between the three regions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgyEF8uqCoe6",
        "outputId": "6f3b1104-4832-474c-ebf3-720117d1b3c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis. There is a statistically significant difference in average heights between the three regions.\n"
          ]
        }
      ]
    }
  ]
}